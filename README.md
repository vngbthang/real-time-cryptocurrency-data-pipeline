# üí∞ Real-Time Cryptocurrency Data Pipeline

## üìã M·ª•c ti√™u (Objective)

Project n√†y x√¢y d·ª±ng m·ªôt **Real-Time ETL Pipeline** ho√†n ch·ªânh theo ki·∫øn tr√∫c **Medallion** (Bronze-Silver-Gold) ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu cryptocurrency t·ª´ Coinbase API, cung c·∫•p analytics v√† insights theo th·ªùi gian th·ª±c cho 5 lo·∫°i cryptocurrency: **BTC, ETH, SOL, ADA, DOGE**.

**V·∫•n ƒë·ªÅ gi·∫£i quy·∫øt:**

- **Real-time ingestion**: Thu th·∫≠p d·ªØ li·ªáu gi√° v√† kh·ªëi l∆∞·ª£ng giao d·ªãch t·ª´ Coinbase API m·ªói 10 gi√¢y.
- **Stream processing**: X·ª≠ l√Ω d·ªØ li·ªáu real-time v·ªõi Spark Structured Streaming.
- **Data aggregation**: T·∫°o metrics theo c·ª≠a s·ªï th·ªùi gian (10 ph√∫t, 1 gi·ªù) cho ph√¢n t√≠ch.
- **Orchestration**: T·ª± ƒë·ªông h√≥a pipeline v·ªõi Apache Airflow.
- **Analytics ready**: Cung c·∫•p d·ªØ li·ªáu s·∫µn s√†ng cho BI tools (Grafana, pgAdmin, REST API).

## üèóÔ∏è Ki·∫øn tr√∫c (Architecture)

![Architecture](docs/images/architecture.png)  <!-- You can create this image later -->

**Ki·∫øn tr√∫c Medallion**: Bronze (Kafka) ‚Üí Silver (Raw Data) ‚Üí Gold (Aggregated Metrics)

**Lu·ªìng d·ªØ li·ªáu chi ti·∫øt:**

```
Coinbase API (5 cryptocurrencies)
    ‚Üì
Producer (Python + kafka-python) - Poll m·ªói 10 gi√¢y
    ‚Üì
Bronze Layer: Kafka Topic (crypto_prices) - Message streaming
    ‚Üì
Spark Structured Streaming - Real-time ETL processing
    ‚Üì
Silver Layer: PostgreSQL (crypto_prices_realtime) - Raw structured data
    ‚Üì
Airflow DAGs - Scheduled aggregation (every 10 minutes)
    ‚Üì
Gold Layer: PostgreSQL (gold_hourly_metrics, gold_10min_metrics) - Analytics tables
    ‚Üì
BI Layer: Grafana Dashboards, REST API, pgAdmin
```

## üõ†Ô∏è C√¥ng ngh·ªá s·ª≠ d·ª•ng (Tech Stack)

| Component             | Technology                      | Version   |
|-----------------------|---------------------------------|-----------|
| **Message Broker**    | Apache Kafka                    | 7.3.0     |
| **Stream Processing** | Apache Spark Structured Streaming | 3.5.0     |
| **Database**          | PostgreSQL                      | 14        |
| **Orchestration**     | Apache Airflow                  | 2.8.1     |
| **Data Source**       | Coinbase API                    | v2        |
| **Producer**          | Python + kafka-python           | 3.11 / 2.0.2 |
| **Container Platform**| Docker + Docker Compose         | Latest    |
| **BI Visualization**  | Grafana (optional)              | Latest    |
| **API Framework**     | FastAPI (optional)              | Latest    |

## üìä C·∫•u tr√∫c D·ªØ li·ªáu (Schema)

### Infrastructure Layout:

```
Docker Containers (11 services):
‚îú‚îÄ‚îÄ zookeeper (Port 2181)
‚îú‚îÄ‚îÄ kafka (Ports 9092 internal, 9093 external)
‚îú‚îÄ‚îÄ kafka-init (Auto-creates topics)
‚îú‚îÄ‚îÄ postgres-db (Port 5432) - Main database
‚îú‚îÄ‚îÄ postgres-airflow-db (Port 5433) - Airflow metadata
‚îú‚îÄ‚îÄ crypto-producer (Containerized producer)
‚îú‚îÄ‚îÄ spark-master (Port 8081)
‚îú‚îÄ‚îÄ spark-worker (Port 8082)
‚îú‚îÄ‚îÄ airflow-webserver (Port 8080)
‚îú‚îÄ‚îÄ airflow-scheduler
‚îî‚îÄ‚îÄ airflow-init
```

### Data Schema:

```
crypto_data (PostgreSQL Database)
‚îÇ
‚îú‚îÄ‚îÄ Bronze Layer (Kafka Topic)
‚îÇ   ‚îî‚îÄ‚îÄ crypto_prices (topic)
‚îÇ       ‚îú‚îÄ‚îÄ Key: symbol (STRING)
‚îÇ       ‚îî‚îÄ‚îÄ Value: JSON {timestamp, symbol, base, currency, price, volume_24h, source, iteration}
‚îÇ
‚îú‚îÄ‚îÄ Silver Layer (Raw Structured Data)
‚îÇ   ‚îî‚îÄ‚îÄ crypto_prices_realtime (table)
‚îÇ       ‚îú‚îÄ‚îÄ timestamp (BIGINT)
‚îÇ       ‚îú‚îÄ‚îÄ symbol (VARCHAR) - e.g., "BTC-USD"
‚îÇ       ‚îú‚îÄ‚îÄ base (VARCHAR) - e.g., "BTC"
‚îÇ       ‚îú‚îÄ‚îÄ currency (VARCHAR) - e.g., "USD"
‚îÇ       ‚îú‚îÄ‚îÄ price (FLOAT)
‚îÇ       ‚îú‚îÄ‚îÄ volume_24h (FLOAT) - 24h trading volume
‚îÇ       ‚îú‚îÄ‚îÄ source (VARCHAR) - "coinbase"
‚îÇ       ‚îú‚îÄ‚îÄ iteration (BIGINT) - Producer iteration number
‚îÇ       ‚îî‚îÄ‚îÄ processed_at (TIMESTAMP) - Spark processing timestamp
‚îÇ
‚îî‚îÄ‚îÄ Gold Layer (Aggregated Analytics)
    ‚îú‚îÄ‚îÄ gold_hourly_metrics (table)
    ‚îÇ   ‚îú‚îÄ‚îÄ hour_timestamp (TIMESTAMP) - Hour bucket
    ‚îÇ   ‚îú‚îÄ‚îÄ symbol (VARCHAR)
    ‚îÇ   ‚îú‚îÄ‚îÄ avg_price (FLOAT)
    ‚îÇ   ‚îú‚îÄ‚îÄ min_price (FLOAT)
    ‚îÇ   ‚îú‚îÄ‚îÄ max_price (FLOAT)
    ‚îÇ   ‚îú‚îÄ‚îÄ total_volume (FLOAT) - Sum of volumes in hour
    ‚îÇ   ‚îú‚îÄ‚îÄ avg_volume (FLOAT) - Average volume in hour
    ‚îÇ   ‚îú‚îÄ‚îÄ price_change (FLOAT) - Change from previous hour
    ‚îÇ   ‚îú‚îÄ‚îÄ price_change_percent (FLOAT) - % change from previous hour
    ‚îÇ   ‚îú‚îÄ‚îÄ record_count (INT) - Number of data points
    ‚îÇ   ‚îî‚îÄ‚îÄ created_at (TIMESTAMP)
    ‚îÇ
    ‚îî‚îÄ‚îÄ gold_10min_metrics (table)
        ‚îú‚îÄ‚îÄ window_start (TIMESTAMP) - 10-minute window start
        ‚îú‚îÄ‚îÄ symbol (VARCHAR)
        ‚îú‚îÄ‚îÄ avg_price (FLOAT)
        ‚îú‚îÄ‚îÄ min_price (FLOAT)
        ‚îú‚îÄ‚îÄ max_price (FLOAT)
        ‚îú‚îÄ‚îÄ total_volume (FLOAT)
        ‚îú‚îÄ‚îÄ avg_volume (FLOAT)
        ‚îú‚îÄ‚îÄ price_volatility (FLOAT) - Standard deviation
        ‚îú‚îÄ‚îÄ record_count (INT)
        ‚îî‚îÄ‚îÄ created_at (TIMESTAMP)
```

### Tracked Cryptocurrencies:

```python
CRYPTO_PAIRS = [
    'BTC-USD',  # Bitcoin
    'ETH-USD',  # Ethereum
    'SOL-USD',  # Solana
    'ADA-USD',  # Cardano
    'DOGE-USD'  # Dogecoin
]
```

## üöÄ C√°ch thi·∫øt l·∫≠p v√† ch·∫°y (Setup & Run)

### Prerequisites:

- Docker Desktop (Windows)
- Docker Compose
- Git
- PowerShell
- Minimum 8GB RAM, 20GB disk space

### B∆∞·ªõc 1: Clone Repository

```powershell
git clone https://github.com/vngbthang/real-time-cryptocurrency-data-pipeline.git
cd real-time-cryptocurrency-data-pipeline
```

### B∆∞·ªõc 2: Kh·ªüi ƒë·ªông Infrastructure

```powershell
# Start all Docker containers
docker-compose up -d

# Verify all containers are running (should see 11 containers)
docker ps
```

**L∆∞u √Ω:** Database schema v√† Kafka topics s·∫Ω ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông khi containers kh·ªüi ƒë·ªông l·∫ßn ƒë·∫ßu:
- ‚úÖ PostgreSQL tables: `crypto_prices_realtime`, `gold_hourly_metrics`, `gold_10min_metrics` (via `init-db.sql`)
- ‚úÖ Kafka topic: `crypto_prices` (via `kafka-init` container)
- ‚úÖ Producer: T·ª± ƒë·ªông start v√† b·∫Øt ƒë·∫ßu g·ª≠i d·ªØ li·ªáu

### B∆∞·ªõc 3: Trigger Spark Streaming Job

```powershell
# Open Airflow UI
Start-Process "http://localhost:8080"

# Login: admin / admin
# Navigate to DAGs -> Find "crypto_streaming_pipeline"
# Click "Trigger DAG" (play icon)
```

### B∆∞·ªõc 4: Enable Gold Layer Aggregation

In the Airflow UI, unpause the following DAGs:
- `gold_hourly_aggregation`
- `gold_10min_aggregation`

### B∆∞·ªõc 5: Ki·ªÉm tra d·ªØ li·ªáu

```powershell
# Check Silver layer
docker exec -it postgres-db psql -U user -d crypto_data -c "SELECT symbol, price, processed_at FROM crypto_prices_realtime ORDER BY processed_at DESC LIMIT 5;"

# Check Gold Hourly Metrics
docker exec -it postgres-db psql -U user -d crypto_data -c "SELECT symbol, ROUND(avg_price::numeric, 2) as avg_price, ROUND(price_change_percent::numeric, 2) as change_pct FROM gold_hourly_metrics ORDER BY hour_timestamp DESC LIMIT 5;"

# Check Gold 10-Minute Metrics
docker exec -it postgres-db psql -U user -d crypto_data -c "SELECT symbol, ROUND(avg_price::numeric, 2) as avg, ROUND(price_volatility::numeric, 2) as volatility FROM gold_10min_metrics ORDER BY window_start DESC LIMIT 5;"
```

For more detailed instructions, see the [Deployment Guide](docs/DEPLOYMENT.md).

## üìà K·∫øt qu·∫£ (Final Output)

### Silver Layer Table: `crypto_prices_realtime`
- **M·ª•c ƒë√≠ch:** Raw structured data from Spark streaming.
- **Use cases:** Real-time price monitoring, data quality checks, raw data for ad-hoc analysis.

### Gold Layer Table 1: `gold_hourly_metrics`
- **M·ª•c ƒë√≠ch:** Hourly aggregated analytics.
- **Use cases:** Historical trend analysis, day-over-day comparisons, hourly performance reports.

### Gold Layer Table 2: `gold_10min_metrics`
- **M·ª•c ƒë√≠ch:** Near real-time analytics with 10-minute windows.
- **Use cases:** Real-time volatility monitoring, short-term trading signals, anomaly detection.

## üìÅ C·∫•u tr√∫c Project

```
real-time-cryptocurrency-data-pipeline/
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml              # Infrastructure orchestration (9 containers)
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ README.md                       # This file
‚îÇ
‚îú‚îÄ‚îÄ coinbase_producer.py            # Multi-coin data producer
‚îÇ
‚îú‚îÄ‚îÄ spark-apps/
‚îÇ   ‚îî‚îÄ‚îÄ spark_stream_processor.py   # Spark Structured Streaming job
‚îÇ
‚îú‚îÄ‚îÄ dags/                           # Airflow orchestration
‚îÇ   ‚îú‚îÄ‚îÄ crypto_producer.py
‚îÇ   ‚îú‚îÄ‚îÄ submit_spark_stream.py
‚îÇ   ‚îú‚îÄ‚îÄ gold_aggregation.py
‚îÇ   ‚îî‚îÄ‚îÄ gold_10min_aggregation.py
‚îÇ
‚îú‚îÄ‚îÄ sql/
‚îÇ   ‚îî‚îÄ‚îÄ alter_tables_add_volume.sql # Database migration script
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md               # Complete deployment guide
‚îÇ   ‚îî‚îÄ‚îÄ BI_INTEGRATION.md           # BI tools integration guide
‚îÇ
‚îî‚îÄ‚îÄ logs/                           # Airflow logs directory
```

## üéØ Key Features

- ‚úÖ **Real-Time Processing**: 10-second polling and 15-second micro-batching.
- ‚úÖ **Multi-Cryptocurrency Support**: Tracks 5 major coins, easily extensible.
- ‚úÖ **Medallion Architecture**: Bronze (Kafka), Silver (PostgreSQL), and Gold (PostgreSQL) layers.
- ‚úÖ **Data Quality & Reliability**: Spark checkpointing, producer retry logic, and data retention policies.
- ‚úÖ **Orchestration & Monitoring**: Fully automated with Airflow and monitored via Spark UI and Airflow UI.
- ‚úÖ **Scalability**: Designed for horizontal and vertical scaling.
- ‚úÖ **Analytics Ready**: Pre-aggregated metrics in the Gold layer for fast BI queries.

## üîß Configuration

Key configuration variables can be found in:
- `coinbase_producer.py` (Kafka servers, poll interval, crypto pairs)
- `spark-apps/spark_stream_processor.py` (Kafka servers, Spark master, trigger interval)
- `docker-compose.yml` (Airflow settings)

## üìö Deployment Guide (H∆∞·ªõng d·∫´n Tri·ªÉn khai Chi ti·∫øt)

### üîç Monitoring & Verification (Gi√°m s√°t & Ki·ªÉm tra)

#### Kafka Topics
```powershell
# Li·ªát k√™ t·∫•t c·∫£ topics
docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --list

# Ki·ªÉm tra messages trong topic crypto_prices
docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic crypto_prices --from-beginning --max-messages 5
```

#### Spark Jobs
```powershell
# Truy c·∫≠p Spark Master UI
Start-Process "http://localhost:8081"

# Xem log Spark Master
docker logs spark-master --tail 50

# Xem log Spark Worker
docker logs spark-worker --tail 50
```

#### Airflow DAGs
```powershell
# Xem danh s√°ch DAGs
docker exec -it airflow-webserver airflow dags list

# Xem task instances
docker exec -it airflow-webserver airflow tasks list crypto_streaming_pipeline
```

#### Database Queries
```powershell
# ƒê·∫øm s·ªë records theo symbol
docker exec -it postgres-db psql -U user -d crypto_data -c "SELECT symbol, COUNT(*) FROM crypto_prices_realtime GROUP BY symbol;"

# Ki·ªÉm tra gi√° m·ªõi nh·∫•t
docker exec -it postgres-db psql -U user -d crypto_data -c "SELECT DISTINCT ON (symbol) symbol, price, processed_at FROM crypto_prices_realtime ORDER BY symbol, processed_at DESC;"

# Ph√¢n t√≠ch volume (khi c√≥ d·ªØ li·ªáu)
docker exec -it postgres-db psql -U user -d crypto_data -c "SELECT symbol, AVG(volume_24h) as avg_volume FROM crypto_prices_realtime WHERE volume_24h IS NOT NULL GROUP BY symbol;"
```

### üõ†Ô∏è Troubleshooting (X·ª≠ l√Ω s·ª± c·ªë)

#### Producer Issues
**V·∫•n ƒë·ªÅ**: L·ªói "NoBrokersAvailable"
```powershell
# Ki·ªÉm tra Kafka c√≥ ch·∫°y kh√¥ng
docker logs kafka | Select-String -Pattern "started"

# Test k·∫øt n·ªëi
Test-NetConnection localhost -Port 9093

# N·∫øu kh√¥ng ƒë∆∞·ª£c, restart Kafka
docker-compose restart kafka
Start-Sleep -Seconds 20
```

**V·∫•n ƒë·ªÅ**: API rate limiting
- TƒÉng `POLL_INTERVAL_SECONDS` trong `coinbase_producer.py` (m·∫∑c ƒë·ªãnh: 10)

#### Spark Job Issues
**V·∫•n ƒë·ªÅ**: Job kh√¥ng x·ª≠ l√Ω d·ªØ li·ªáu
```powershell
# Ki·ªÉm tra log Spark
docker logs spark-master
docker logs spark-worker

# Ki·ªÉm tra k·∫øt n·ªëi Kafka t·ª´ Spark
docker exec -it spark-master nc -zv kafka 9092
```

**V·∫•n ƒë·ªÅ**: Checkpoint b·ªã h·ªèng
```powershell
# X√≥a checkpoints v√† restart
docker exec -it spark-master rm -rf /opt/spark/apps/checkpoints/*
```

#### Database Issues
**V·∫•n ƒë·ªÅ**: Connection refused
```powershell
# Ki·ªÉm tra PostgreSQL ƒëang ch·∫°y
docker exec -it postgres-db pg_isready

# Ki·ªÉm tra logs
docker logs postgres-db
```

**V·∫•n ƒë·ªÅ**: Schema mismatch
```powershell
# Ch·∫°y l·∫°i migration script
docker cp sql/alter_tables_add_volume.sql postgres-db:/tmp/
docker exec -it postgres-db psql -U user -d crypto_data -f /tmp/alter_tables_add_volume.sql
```

### üìà Next Steps (B∆∞·ªõc ti·∫øp theo)

#### Monitoring Producer Container
```powershell
# Xem logs c·ªßa producer
docker logs crypto-producer --tail 50 -f

# Ki·ªÉm tra producer ƒëang ch·∫°y
docker exec crypto-producer ps aux

# Restart producer n·∫øu c·∫ßn
docker-compose restart crypto-producer
```

#### Th√™m Volume Data Th·ª±c t·∫ø
Hi·ªán t·∫°i `volume_24h` ƒëang l√† `None` v√¨ Coinbase API v2 `/spot` endpoint kh√¥ng cung c·∫•p volume. ƒê·ªÉ th√™m volume th·ª±c:

**Option 1: S·ª≠ d·ª•ng Coinbase Advanced Trade API**
```python
# C·∫ßn x√°c th·ª±c
COINBASE_PRODUCT_API = 'https://api.coinbase.com/api/v3/brokerage/products/{pair}/ticker'
# Tr·∫£ v·ªÅ: price, volume_24h, price_percent_change_24h
```

**Option 2: S·ª≠ d·ª•ng CoinGecko API (kh√¥ng c·∫ßn x√°c th·ª±c)**
```python
COINGECKO_API = 'https://api.coingecko.com/api/v3/simple/price'
# Parameters: ids=bitcoin,ethereum&vs_currencies=usd&include_24hr_vol=true
```

C·∫≠p nh·∫≠t `coinbase_producer.py` ƒë·ªÉ l·∫•y volume data t·ª´ CoinGecko ho·∫∑c Advanced Trade API.

---

## üîå BI Integration Guide (H∆∞·ªõng d·∫´n T√≠ch h·ª£p BI)

### üìä K·∫øt n·ªëi Database

#### Th√¥ng tin k·∫øt n·ªëi PostgreSQL
```
Host: localhost
Port: 5432
Database: crypto_data
Username: user
Password: password
```

### Power BI / Tableau

#### B∆∞·ªõc 1: Ch·ªçn Data Source
- M·ªü Power BI Desktop ho·∫∑c Tableau
- Ch·ªçn "PostgreSQL" l√†m data source
- Nh·∫≠p th√¥ng tin k·∫øt n·ªëi ·ªü tr√™n

#### B∆∞·ªõc 2: Select Tables
Ch·ªçn c√°c b·∫£ng:
- `crypto_prices_realtime` (Silver Layer) - Real-time data
- `gold_hourly_metrics` (Gold Layer) - Hourly analytics
- `gold_10min_metrics` (Gold Layer) - 10-minute analytics

#### B∆∞·ªõc 3: T·∫°o Visualizations

**Dashboard 1: Real-Time Price Monitor**
```sql
SELECT 
    symbol,
    price,
    processed_at,
    LAG(price) OVER (PARTITION BY symbol ORDER BY processed_at) as prev_price,
    ROUND(((price - LAG(price) OVER (PARTITION BY symbol ORDER BY processed_at)) / 
           LAG(price) OVER (PARTITION BY symbol ORDER BY processed_at) * 100)::numeric, 2) as pct_change
FROM crypto_prices_realtime
WHERE processed_at >= NOW() - INTERVAL '1 hour'
ORDER BY processed_at DESC;
```

**Dashboard 2: Hourly Trend Analysis**
```sql
SELECT 
    hour_timestamp,
    symbol,
    avg_price,
    min_price,
    max_price,
    price_change_percent,
    total_volume
FROM gold_hourly_metrics
WHERE hour_timestamp >= NOW() - INTERVAL '24 hours'
ORDER BY hour_timestamp DESC;
```

**Dashboard 3: Volatility Monitor**
```sql
SELECT 
    window_start,
    symbol,
    avg_price,
    price_volatility,
    (max_price - min_price) as price_range,
    ROUND(((max_price - min_price) / avg_price * 100)::numeric, 2) as volatility_pct
FROM gold_10min_metrics
WHERE window_start >= NOW() - INTERVAL '2 hours'
ORDER BY window_start DESC;
```

### pgAdmin (Database Management)

#### Setup pgAdmin
```powershell
# Pull pgAdmin image
docker pull dpage/pgadmin4

# Run pgAdmin container
docker run -d `
  --name pgadmin `
  --network crypto-pipeline-net `
  -p 5050:80 `
  -e PGADMIN_DEFAULT_EMAIL=admin@admin.com `
  -e PGADMIN_DEFAULT_PASSWORD=admin `
  dpage/pgadmin4
```

#### Truy c·∫≠p pgAdmin
1. M·ªü browser: http://localhost:5050
2. Login: `admin@admin.com` / `admin`
3. Add New Server:
   - Name: `Crypto Pipeline`
   - Host: `postgres-db`
   - Port: `5432`
   - Username: `user`
   - Password: `password`

### Grafana (Optional)

#### Setup Grafana
```powershell
# Pull Grafana image
docker pull grafana/grafana

# Run Grafana container
docker run -d `
  --name grafana `
  --network crypto-pipeline-net `
  -p 3000:3000 `
  grafana/grafana
```

#### Configure Grafana
1. Truy c·∫≠p: http://localhost:3000
2. Login: `admin` / `admin`
3. Add PostgreSQL Data Source:
   - Host: `postgres-db:5432`
   - Database: `crypto_data`
   - User: `user`
   - Password: `password`
   - SSL Mode: `disable`

#### Sample Grafana Queries

**Panel 1: Current Prices**
```sql
SELECT 
  processed_at as time,
  symbol as metric,
  price as value
FROM crypto_prices_realtime
WHERE $__timeFilter(processed_at)
ORDER BY processed_at;
```

**Panel 2: Hourly Average Prices**
```sql
SELECT 
  hour_timestamp as time,
  symbol as metric,
  avg_price as value
FROM gold_hourly_metrics
WHERE $__timeFilter(hour_timestamp)
ORDER BY hour_timestamp;
```

**Panel 3: Price Volatility**
```sql
SELECT 
  window_start as time,
  symbol as metric,
  price_volatility as value
FROM gold_10min_metrics
WHERE $__timeFilter(window_start)
ORDER BY window_start;
```

### REST API (FastAPI - Optional)

T·∫°o file `api/main.py`:
```python
from fastapi import FastAPI
import psycopg2
from psycopg2.extras import RealDictCursor

app = FastAPI()

DB_CONFIG = {
    "host": "localhost",
    "port": 5432,
    "database": "crypto_data",
    "user": "user",
    "password": "password"
}

@app.get("/api/prices/latest")
def get_latest_prices():
    conn = psycopg2.connect(**DB_CONFIG, cursor_factory=RealDictCursor)
    cur = conn.cursor()
    cur.execute("""
        SELECT DISTINCT ON (symbol) 
            symbol, price, processed_at
        FROM crypto_prices_realtime
        ORDER BY symbol, processed_at DESC;
    """)
    results = cur.fetchall()
    cur.close()
    conn.close()
    return results

@app.get("/api/metrics/hourly/{symbol}")
def get_hourly_metrics(symbol: str, hours: int = 24):
    conn = psycopg2.connect(**DB_CONFIG, cursor_factory=RealDictCursor)
    cur = conn.cursor()
    cur.execute("""
        SELECT * FROM gold_hourly_metrics
        WHERE symbol = %s 
        AND hour_timestamp >= NOW() - INTERVAL '%s hours'
        ORDER BY hour_timestamp DESC;
    """, (symbol, hours))
    results = cur.fetchall()
    cur.close()
    conn.close()
    return results
```

Ch·∫°y API:
```powershell
pip install fastapi uvicorn psycopg2-binary
uvicorn api.main:app --reload --port 8000
```

Truy c·∫≠p API docs: http://localhost:8000/docs

---

## üìö T√†i li·ªáu tham kh·∫£o (References)

- [Coinbase API Documentation](https://docs.cloud.coinbase.com/sign-in-with-coinbase/docs/api-prices)
- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Airflow Documentation](https://airflow.apache.org/docs/apache-airflow/stable/)
- [Kafka Docker Setup](https://developer.confluent.io/quickstart/kafka-docker/)
